#!/usr/bin/python3

import os
import sys
import argparse
import subprocess
import datetime

import perfscrape
import htmconfig

###############################################################################
# Initialization
###############################################################################

def parseArguments():
    desc = "Automagically tune placement of migration points/HTM " \
           "transactions in an application by observing perf output of HTM " \
           "counters.  NOTE: the build environment must be set up to accept " \
           "environment variables CAP_THRESH, START_THRESH, RET_THRESH, and " \
           "OTHER to configure the various thresholds and add additional " \
           "arguments, respectively."

    parser = argparse.ArgumentParser(description=desc,
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    running = parser.add_argument_group("Running")
    running.add_argument("-binary", type=str, required=True,
        help="Name of binary generated by build command",
        dest="binary")
    running.add_argument("-build-cmd", type=str, required=True,
        help="Command to build the binary, we'll append the configuration",
        dest="buildCmd")
    running.add_argument("-run-cmd", type=str, required=True,
        help="Command to run the binary",
        dest="runCmd")
    running.add_argument("-perf", type=str, help="Which version of perf to use",
        default="~/perf/perf",
        dest="perf")
    running.add_argument("-htm-perf", type=str,
        help="Command to run the HTM profiling script",
        default="htm-perf.sh",
        dest="htmPerf")
    running.add_argument("-clean-cmd", type=str,
        help="Command to clean the build",
        default="make clean",
        dest="cleanCmd")

    tuning = parser.add_argument_group("Tuning")
    tuning.add_argument("-max-iter", type=int,
        help="Maximum number of iterations to explore configuration",
        default=50,
        dest="maxIters")
    tuning.add_argument("-max-func-iter", type=int,
        help="Maximum number of iterations to tune a single function",
        default=10,
        dest="maxFuncIters")
    tuning.add_argument("-target-time", type=float, required=True,
        help="Time to run uninstrumented version of the application (used for " \
             "determining when to stop)",
        dest="targetTime")
    tuning.add_argument("-stop-thresh", type=int,
        help="Percent slowdown compared to target time at which stop " \
             "configuration exploration",
        default=5,
        dest="slowdownThresh")

    return parser.parse_args()

def sanityCheckArgs(args):
    args.perf = os.path.abspath(args.perf)
    args.htmPerf = os.path.abspath(args.htmPerf)

    assert os.path.exists(args.perf), \
           "Perf binary '{}' doesn't exist!".format(args.perf)
    assert os.path.exists(args.htmPerf), \
           "HTM profiling script '{}' doesn't exist!".format(args.htmPerf)
    assert args.maxIters > 0 and args.maxIters < 100, \
           "Invalid maximum number of iterations"
    assert args.maxFuncIters > 0 and args.maxFuncIters <= args.maxIters, \
           "Invalid maximum number of per-function iterations"
    assert targetTime > 0, "Invalid target runtime"
    assert slowdownThresh > 0, "Invalid slowdown percentage threshold"

def writeReadme(readme, args):
    readme.write("Results for '{}':\n".format(args.binary))
    readme.write("  Build command: '{}'\n".format(args.buildCmd))
    readme.write("  Run command: '{}'\n".format(args.runCmd))
    readme.write("  Perf binary: {}\n".format(args.perf))
    readme.write("  HTM profiling script: {}\n".format(args.htmPerf))
    readme.write("  Clean command: '{}'\n\n".format(args.cleanCmd))

    readme.write("Tuning configuration:\n");
    readme.write("  Max iterations: {}\n".format(args.maxIters))
    readme.write("  Max iterations per function: {}\n".format(
        args.maxFuncIters))
    readme.write("  Target time: {}\n".format(args.targetTime))
    readme.write("  Stop threshold: {}\n".format(args.slowdownThresh))

def initialize(args):
    sanityCheckArgs(args)
    now = datetime.datetime.now()
    results = "htm-instrument-{}:{}-{}-{}-{}".format(
        now.hour, now.minute, now.day, now.month, now.year)
    assert not os.path.exists(results), \
           "Tuning output folder '{}' already exists!".format(results)
    os.makedirs(results)
    with open(results + "/README", 'w') as readme: writeReadme(readme, args)
    return results

###############################################################################
# Generating & running the binary
###############################################################################

# Run the clean command to remove previous builds.
def cleanBuild(cleanCmd):
    try:
        args = cleanCmd.strip().split()
        rv = subprocess.check_call(args, stderr=subprocess.STDOUT)
    except Exception as e:
        print("ERROR: could not clean in preparation for building ({})!" \
              .format(e))
        sys.exit(1)
    else:
        if rv != 0:
            print("ERROR: clean command returned non-zero!")
            sys.exit(1)

# Run the build command to generate a binary with a given configuration.
def buildBinary(buildCmd, binary, cap, start, ret, func):
    def funcThreshArgs(funcConfig):
        conf = ""
        for func in funcConfig:
            conf += " -mllvm -func-cap={},{}".format(func, funcConfig[func])
        return conf

    try:
        args = buildCmd.strip().split()
        args.append("CAP_THRESH={}".format(cap))
        args.append("START_THRESH={}".format(start))
        args.append("RET_THRESH={}".format(ret))
        args.append("OTHER=\"{}\"".format(funcThreshArgs(func)))
        rv = subprocess.check_call(args, stderr=subprocess.STDOUT)
    except Exception as e:
        print("Could not build the binary:\n{}".format(e))
        sys.exit(1)
    else:
        if rv != 0:
            print("Build command returned non-zero!")
            sys.exit(1)

    assert os.path.isfile(binary), \
           "Binary '{}' does not exist after build!".format(binary)

def runBinary(it, outputFolder, runCmd, perf, htmPerf, binary):
    try:
        args = [ htmPerf, "-p", perf, "--" ]
        args.extend(runCmd.strip().split())
        rv = subprocess.check_call(args, stderr=subprocess.STDOUT)
    except Exception as e:
        print("Could not run the binary ({})!".format(e))
        sys.exit(1)

    # The run command should have generated a counters file (*.log) and a
    # sampling file (*.data)
    statOutput = "{}.log".format(binary)
    recordOutput = "{}.data".format(binary)
    assert os.path.isfile(statOutput), \
           "perf-stat output {} does not exist after run!".format(statOutput)
    assert os.path.isfile(recordOutput), \
           "perf-record output {} does not exist after run!" \
           .format(recordOutput)
    statDest = outputFolder + '/' + os.path.basename(statOutput)
    recordDest = outputFolder + '/' + os.path.basename(recordOutput)
    os.rename(statOutput, statOutput)
    os.rename(recordOutput, recordOutput)
    return statDest, recordDest

def runConfiguration(args, iteration, results, cap, start, ret, func):
    # Make an output folder for the current iteration.
    iterDir = results + "/" + str(iteration)
    os.makedirs(iterDir)

    # Clean/build/run the current configuration
    cleanBuild(args.cleanCmd)
    buildBinary(args.buildCmd, args.binary, cap, start, ret, func)
    return runBinary(it, iterDir, args.runCmd, args.perf,
                     args.htmPerf, args.binary)

###############################################################################
# Driver
###############################################################################

if __name__ == "__main__":
    args = parseArguments()
    results = initialize(args)
    marcoPolo = ConfigureHTM(args.targetTime, args.slowdownThresh,
                             args.maxIters, args.maxFuncIters, results)

    while marcoPolo.keepGoing:
        cap, start, ret, func = marcoPolo.getConfiguration()
        stat, record = runConfiguration(args, marcoPolo.iter, results, \
                                        cap, start, ret, func)
        time, counters = scrapePerfStat(stat)
        numSamples, eventCount, samples = scrapePerfReport(args.perf, record)
        marcoPolo.analyze(time, counters, numSamples, samples)

    marcoPolo.writeBest()

